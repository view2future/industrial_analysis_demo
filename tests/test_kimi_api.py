#!/usr/bin/env python3
"""
æµ‹è¯• Kimi API é›†æˆ
ç”¨äºéªŒè¯ API é…ç½®æ˜¯å¦æ­£ç¡®
"""

import sys
import os
import json
import logging
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_api_connection():
    """æµ‹è¯• Kimi API åŸºç¡€è¿æ¥"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯• Kimi API è¿æ¥")
    print("="*80 + "\n")
    
    try:
        from openai import OpenAI
        print("âœ“ openai åº“å¯¼å…¥æˆåŠŸ")
        
        # åŠ è½½é…ç½®
        with open('config.json', 'r') as f:
            config = json.load(f)
        
        api_key = config['api_keys']['kimi_api_key']
        print(f"\nâœ“ API Key å·²åŠ è½½")
        print(f"  å‰10ä½: {api_key[:10]}...")
        print(f"  é•¿åº¦: {len(api_key)} å­—ç¬¦")
        
        # é…ç½®å®¢æˆ·ç«¯
        print(f"\nâ³ åˆå§‹åŒ– Kimi å®¢æˆ·ç«¯...")
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.moonshot.cn/v1"
        )
        print("âœ“ å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç®€å•è°ƒç”¨
        print(f"\nâ³ æµ‹è¯• API è°ƒç”¨...")
        print(f"  æ¨¡å‹: moonshot-v1-8k")
        print(f"  æç¤ºè¯: 'ä½ å¥½ï¼Œè¯·è¯´\"æµ‹è¯•æˆåŠŸ\"'")
        
        import time
        start_time = time.time()
        
        completion = client.chat.completions.create(
            model="moonshot-v1-8k",
            messages=[
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·è¯´'æµ‹è¯•æˆåŠŸ'"}
            ],
            temperature=0.3,
            max_tokens=50
        )
        
        elapsed = time.time() - start_time
        
        response_text = completion.choices[0].message.content
        
        print(f"\nâœ… API è°ƒç”¨æˆåŠŸï¼")
        print(f"  è€—æ—¶: {elapsed:.2f} ç§’")
        print(f"  å“åº”å†…å®¹: {response_text}")
        print(f"  Token ä½¿ç”¨:")
        print(f"    - Prompt: {completion.usage.prompt_tokens}")
        print(f"    - Completion: {completion.usage.completion_tokens}")
        print(f"    - Total: {completion.usage.total_tokens}")
        
        print("\n" + "="*80)
        print("ğŸ‰ Kimi API é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        print("="*80 + "\n")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼")
        print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"  é”™è¯¯ä¿¡æ¯: {str(e)}")
        
        import traceback
        print(f"\nè¯¦ç»†é”™è¯¯:")
        traceback.print_exc()
        
        return False


def test_llm_generator():
    """æµ‹è¯• LLM æŠ¥å‘Šç”Ÿæˆå™¨"""
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯• Kimi LLM æŠ¥å‘Šç”Ÿæˆå™¨")
    print("="*80 + "\n")
    
    try:
        from src.ai.llm_generator import LLMReportGenerator
        
        print("â³ åˆå§‹åŒ– LLM æŠ¥å‘Šç”Ÿæˆå™¨...")
        generator = LLMReportGenerator()
        
        print("\nâ³ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        print("  åŸå¸‚: æˆéƒ½")
        print("  è¡Œä¸š: äººå·¥æ™ºèƒ½")
        print("  è¦æ±‚: ç®€çŸ­æµ‹è¯•æŠ¥å‘Šï¼Œ500å­—ä»¥å†…")
        
        result = generator.generate_report(
            city="æˆéƒ½",
            industry="äººå·¥æ™ºèƒ½",
            additional_context="è¿™æ˜¯ä¸€ä¸ªAPIæµ‹è¯•ï¼Œè¯·ç”Ÿæˆä¸€ä»½ç®€çŸ­çš„æµ‹è¯•æŠ¥å‘Šï¼Œ500å­—ä»¥å†…å³å¯ã€‚"
        )
        
        if result.get('success'):
            print(f"\nâœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
            print(f"  æŠ¥å‘Šé•¿åº¦: {len(result['full_content'])} å­—ç¬¦")
            print(f"  ç« èŠ‚æ•°: {len(result['sections'])}")
            print(f"  ç« èŠ‚åˆ—è¡¨: {list(result['sections'].keys())}")
            
            # Token ä½¿ç”¨æƒ…å†µ
            tokens = result['metadata'].get('tokens', {})
            if tokens:
                print(f"\n  Token ä½¿ç”¨:")
                print(f"    - Prompt: {tokens.get('prompt', 0)}")
                print(f"    - Completion: {tokens.get('completion', 0)}")
                print(f"    - Total: {tokens.get('total', 0)}")
            
            print(f"\n  æŠ¥å‘Šé¢„è§ˆ (å‰300å­—):")
            print(f"  {result['full_content'][:300]}...")
            
            print("\n" + "="*80)
            print("ğŸ‰ Kimi LLM æŠ¥å‘Šç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡ï¼")
            print("="*80 + "\n")
            return True
        else:
            print(f"\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼")
            print(f"  é”™è¯¯: {result.get('error')}")
            return False
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼")
        print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"  é”™è¯¯ä¿¡æ¯: {str(e)}")
        
        import traceback
        print(f"\nè¯¦ç»†é”™è¯¯:")
        traceback.print_exc()
        
        return False


if __name__ == '__main__':
    print("\n")
    print("â–ˆ" * 80)
    print("  Kimi API é›†æˆæµ‹è¯•å·¥å…·")
    print("â–ˆ" * 80)
    
    # æµ‹è¯• 1: API è¿æ¥
    api_ok = test_api_connection()
    
    if api_ok:
        # æµ‹è¯• 2: LLM æŠ¥å‘Šç”Ÿæˆå™¨
        generator_ok = test_llm_generator()
        
        if generator_ok:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ã€‚\n")
            print("ğŸ’¡ æç¤º: Kimi API æ— éœ€ä»£ç†ï¼Œå¯ç›´æ¥è®¿é—®\n")
            sys.exit(0)
        else:
            print("\nâš ï¸  API è¿æ¥æ­£å¸¸ï¼Œä½†æŠ¥å‘Šç”Ÿæˆå™¨æœ‰é—®é¢˜ã€‚\n")
            sys.exit(1)
    else:
        print("\nâŒ API è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
        print("   1. API Key æ˜¯å¦æ­£ç¡®")
        print("   2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("   3. config.json é…ç½®æ˜¯å¦æ­£ç¡®\n")
        sys.exit(1)
