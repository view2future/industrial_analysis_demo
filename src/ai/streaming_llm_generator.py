#!/usr/bin/env python3
"""
Streaming LLM Report Generator with real-time content streaming support
Provides streaming capabilities for Google Gemini and Kimi APIs
"""

import os
import json
import logging
import time
import asyncio
from typing import Dict, Optional, AsyncIterator, Iterator
from pathlib import Path
from openai import OpenAI
import google.generativeai as genai
from src.utils.api_error_handler import api_error_handler, handle_api_error, APIError, APIService

logger = logging.getLogger(__name__)


class StreamingLLMReportGenerator:
    """Streaming LLM Report Generator with real-time content streaming support"""
    
    def __init__(self, config_path='config.json', llm_service: str = 'kimi', enable_fallback=True):
        """Initialize the streaming LLM report generator
        
        Args:
            config_path: Path to configuration file containing API keys
            llm_service: The LLM service to use ('kimi', 'gemini', or 'doubao')
            enable_fallback: Whether to enable service fallback on failures
        """
        logger.info("="*60)
        logger.info(f"åˆå§‹åŒ– {llm_service.upper()} æµå¼ LLM æŠ¥å‘Šç”Ÿæˆå™¨")
        logger.info("="*60)
        
        self.config = self._load_config(config_path)
        self.llm_service = llm_service
        self.enable_fallback = enable_fallback
        self.api_error_handler = api_error_handler
        self.available_services = self._detect_available_services()
        self.current_service = self._get_service_enum(llm_service)
        
        # Streaming configuration
        self.chunk_size = 1024  # Stream chunk size in characters
        self.streaming_timeout = 30  # Timeout for streaming in seconds
        # Simple rate limiter state
        self._last_call_ts = {}
        self._min_interval_sec = {
            APIService.KIMI: float(os.environ.get('KIMI_MIN_INTERVAL', 0.5)),
            APIService.GEMINI: float(os.environ.get('GEMINI_MIN_INTERVAL', 0.5)),
        }
        
        # Initialize clients
        logger.info(f"Initializing clients for service: {self.llm_service}")
        self._initialize_clients()
        
        logger.info(f"âœ… {self.llm_service.upper()} æµå¼ LLM æŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"Available services: {[s.value for s in self.available_services]}")
        logger.info(f"Current service: {self.current_service.value}")
    
    def _get_service_enum(self, service_name: str) -> APIService:
        """Convert service name to enum"""
        try:
            return APIService(service_name.lower())
        except ValueError:
            logger.warning(f"æœªçŸ¥æœåŠ¡ç±»å‹: {service_name}ï¼Œé»˜è®¤ä½¿ç”¨ Kimi")
            return APIService.KIMI
    
    def _detect_available_services(self) -> list:
        """Detect which services have valid API keys configured"""
        available = []
        
        api_keys_cfg = self.config.get('api_keys', {})
        if api_keys_cfg.get('kimi') or api_keys_cfg.get('kimi_api_key') or os.environ.get('KIMI_API_KEY') or os.environ.get('MOONSHOT_API_KEY'):
            available.append(APIService.KIMI)
        
        if api_keys_cfg.get('google_gemini') or api_keys_cfg.get('google_gemini_api_key') or os.environ.get('GOOGLE_GEMINI_API_KEY'):
            available.append(APIService.GEMINI)
        
        if api_keys_cfg.get('doubao_api_key'):
            available.append(APIService.DOUBAO)
        
        logger.info(f"âœ… æ£€æµ‹åˆ°å¯ç”¨æœåŠ¡: {[s.value for s in available]}")
        return available
    
    def _load_config(self, config_path: str) -> Dict:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading config: {e}")
            return {}
    
    def _initialize_clients(self):
        """Initialize API clients for all available services"""
        self.clients = {}
        
        # Initialize Kimi client
        if APIService.KIMI in self.available_services:
            try:
                api_keys_cfg = self.config.get('api_keys', {})
                logger.info(f"API keys config: {api_keys_cfg}")
                api_key = (
                    api_keys_cfg.get('kimi')
                    or api_keys_cfg.get('kimi_api_key')
                    or os.environ.get('KIMI_API_KEY')
                    or os.environ.get('MOONSHOT_API_KEY')
                )
                logger.info(f"Selected Kimi API key (first 10 chars): {api_key[:10] if api_key else 'None'}")
                if api_key:
                    self.clients[APIService.KIMI] = OpenAI(
                        api_key=api_key,
                        base_url="https://api.moonshot.cn/v1"
                    )
                    logger.info("âœ… Kimi å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Kimi å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.warning("æœªæ£€æµ‹åˆ°Kimiå¯†é’¥ï¼ŒKimiä¸å¯ç”¨")
        
        # Initialize Gemini client
        if APIService.GEMINI in self.available_services:
            try:
                api_keys_cfg = self.config.get('api_keys', {})
                api_key = (
                    api_keys_cfg.get('google_gemini')
                    or api_keys_cfg.get('google_gemini_api_key')
                    or os.environ.get('GOOGLE_GEMINI_API_KEY')
                )
                if api_key:
                    genai.configure(api_key=api_key)
                    self.clients[APIService.GEMINI] = genai.GenerativeModel('gemini-1.5-flash-latest')
                    logger.info("âœ… Gemini å®¢æˆ·ç«¯(gemini-1.5-flash-latest)åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ Gemini å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.info("Geminiæœªé…ç½®æˆ–ä¸å¯ç”¨")
        
        # Initialize Doubao client (placeholder)
        if APIService.DOUBAO in self.available_services:
            self.clients[APIService.DOUBAO] = None
            logger.info("âš ï¸ è±†åŒ…å¤§æ¨¡å‹å®¢æˆ·ç«¯ä¸ºå ä½ç¬¦")
    
    async def generate_report_streaming(self, city: str, industry: str, 
                                       additional_context: str = "") -> AsyncIterator[Dict]:
        """Generate report with streaming support, yielding content chunks in real-time
        
        Args:
            city: Target city name
            industry: Target industry name
            additional_context: Additional context or requirements
            
        Yields:
            Dictionary containing streaming data:
            - type: 'start', 'chunk', 'complete', 'error'
            - content: Text content (for chunks)
            - stage: Current processing stage
            - metadata: Additional information
        """
        try:
            logger.info("="*60)
            logger.info(f"ğŸš€ å¼€å§‹æµå¼ç”ŸæˆæŠ¥å‘Š: {city} - {industry}")
            logger.info("="*60)
            
            # Prepare prompt
            prompt = self._prepare_prompt(city, industry, additional_context)
            
            # Yield start signal
            yield {
                'type': 'start',
                'stage': 'generating',
                'message': 'å¼€å§‹ç”ŸæˆæŠ¥å‘Šä¸»ä½“...',
                'metadata': {
                    'city': city,
                    'industry': industry,
                    'service': self.llm_service
                }
            }
            
            # Try to generate with current service
            async for chunk in self._generate_with_service_streaming(
                self.current_service, city, industry, prompt
            ):
                yield chunk
                
                # If we got a complete report, also generate summary and SWOT
                if chunk['type'] == 'complete':
                    full_content = chunk['content']
                    
                    # Generate summaries
                    async for summary_chunk in self._generate_summaries_streaming(full_content, city, industry):
                        yield summary_chunk
                    
                    # Generate SWOT analysis
                    async for swot_chunk in self._generate_swot_streaming(full_content, city, industry):
                        yield swot_chunk
                    
                    break
            
            logger.info("âœ… æµå¼æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æµå¼æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            api_error = handle_api_error(e, self.llm_service, "æµå¼æŠ¥å‘Šç”Ÿæˆ")
            
            yield {
                'type': 'error',
                'error': str(e),
                'api_error': {
                    'type': api_error.error_type.value,
                    'user_message': api_error.user_friendly_message,
                    'suggested_action': api_error.suggested_action
                }
            }
    
    async def _generate_with_service_streaming(self, service: APIService, city: str, industry: str, 
                                             prompt: str) -> AsyncIterator[Dict]:
        """Generate content using the specified service with streaming"""
        
        service_name = service.value
        logger.info(f"ğŸŒ å¼€å§‹æµå¼è°ƒç”¨ {service_name.upper()} API...")
        
        try:
            if service == APIService.KIMI:
                async for chunk in self._stream_kimi(prompt):
                    yield chunk
            elif service == APIService.GEMINI:
                async for chunk in self._stream_gemini(prompt):
                    yield chunk
            elif service == APIService.DOUBAO:
                async for chunk in self._stream_doubao(prompt):
                    yield chunk
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æµå¼æœåŠ¡: {service_name}")
                
        except Exception as e:
            logger.error(f"âŒ {service_name.upper()} æµå¼è°ƒç”¨å¤±è´¥: {e}")
            
            # If fallback is enabled, try other services
            if self.enable_fallback:
                fallback_service = self.api_error_handler.get_fallback_service(service, self.available_services)
                if fallback_service:
                    logger.info(f"ğŸ”„ å›é€€åˆ° {fallback_service.value.upper()} æœåŠ¡...")
                    
                    yield {
                        'type': 'service_fallback',
                        'original_service': service_name,
                        'fallback_service': fallback_service.value,
                        'message': f'æ­£åœ¨åˆ‡æ¢åˆ° {fallback_service.value.upper()} æœåŠ¡...'
                    }
                    
                    async for chunk in self._generate_with_service_streaming(fallback_service, city, industry, prompt):
                        yield chunk
                else:
                    raise Exception("æ²¡æœ‰å¯ç”¨çš„å›é€€æœåŠ¡")
            else:
                raise e
    
    async def _stream_kimi(self, prompt: str) -> AsyncIterator[Dict]:
        """Stream content from Kimi API using OpenAI-compatible streaming"""
        logger.info("ğŸŒ™ å¼€å§‹æµå¼è°ƒç”¨ Kimi API (OpenAI-compatible streaming)...")
        
        client = self.clients.get(APIService.KIMI)
        if not client:
            raise ValueError("Kimi å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        try:
            # Basic rate limiting: ensure minimal interval between calls
            now = time.time()
            last = self._last_call_ts.get(APIService.KIMI, 0)
            min_interval = self._min_interval_sec.get(APIService.KIMI, 0.5)
            if now - last < min_interval:
                await asyncio.sleep(min_interval - (now - last))
            self._last_call_ts[APIService.KIMI] = time.time()
            # Start streaming with OpenAI-compatible API
            attempts = 0
            err = None
            stream = None
            kimi_model = os.environ.get('KIMI_MODEL', 'moonshot-v1-128k')
            kimi_temp = float(os.environ.get('KIMI_TEMPERATURE', 0.7))
            kimi_max = int(os.environ.get('KIMI_MAX_TOKENS', 8000))
            while attempts < 2:
                attempts += 1
                try:
                    stream = client.chat.completions.create(
                        model=kimi_model,
                        messages=[
                            {
                                "role": "system",
                                "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äº§ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿æ’°å†™æ·±åº¦çš„åŒºåŸŸäº§ä¸šåˆ†ææŠ¥å‘Šã€‚è¯·åŸºäºç”¨æˆ·æä¾›çš„æ¡†æ¶å’Œè¦æ±‚ï¼Œç”Ÿæˆè¯¦å®ã€ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šï¼Œæ–‡å­—é•¿åº¦åœ¨5000å­—ä»¥ä¸Šã€‚"
                            },
                            {
                                "role": "user",
                                "content": prompt
                            }
                        ],
                        temperature=kimi_temp,
                        max_tokens=kimi_max,
                        stream=True,
                        stream_options={
                            "include_usage": True
                        }
                    )
                    err = None
                    break
                except Exception as e:
                    err = e
                    await asyncio.sleep(1.0)
            if err and stream is None:
                raise err
            
            accumulated_content = ""
            chunk_count = 0
            
            # Process streaming response
            for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        content = delta.content
                        accumulated_content += content
                        chunk_count += 1
                        
                        # Add small delay to simulate natural streaming pace
                        await asyncio.sleep(0.03)
                        
                        # Yield streaming chunk
                        yield {
                            'type': 'chunk',
                            'content': content,
                            'accumulated': accumulated_content,
                            'chunk_index': chunk_count,
                            'stage': 'generating',
                            'timestamp': time.time()
                        }
            
            # Yield completion
            yield {
                'type': 'complete',
                'content': accumulated_content,
                'stage': 'generating',
                'metadata': {
                    'chunks': chunk_count,
                    'service': 'kimi',
                    'total_length': len(accumulated_content)
                }
            }
            
            logger.info(f"âœ… Kimi æµå¼è°ƒç”¨å®Œæˆï¼Œå…± {chunk_count} ä¸ªåˆ†å—ï¼Œæ€»é•¿åº¦: {len(accumulated_content)}")
            
        except Exception as e:
            logger.error(f"âŒ Kimi æµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise e
    
    async def _stream_gemini(self, prompt: str) -> AsyncIterator[Dict]:
        """Stream content from Google Gemini API using streamGenerateContent"""
        logger.info("ğŸ¯ å¼€å§‹æµå¼è°ƒç”¨ Google Gemini API (streamGenerateContent)...")
        
        client = self.clients.get(APIService.GEMINI)
        if not client:
            raise ValueError("Gemini å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        try:
            # Configure generation
            generation_config = {
                'temperature': 0.7,
                'max_output_tokens': 8000,
                'top_p': 0.8,
                'top_k': 40
            }
            
            # Start streaming generation - using the correct streamGenerateContent method
            response = client.generate_content(
                prompt,
                generation_config=generation_config,
                stream=True  # This enables streamGenerateContent
            )
            
            accumulated_content = ""
            chunk_count = 0
            
            # Process streaming response
            for chunk in response:
                if hasattr(chunk, 'text') and chunk.text:
                    content = chunk.text
                    accumulated_content += content
                    chunk_count += 1
                    
                    # Add small delay to simulate natural streaming pace
                    await asyncio.sleep(0.05)
                    
                    # Yield streaming chunk
                    yield {
                        'type': 'chunk',
                        'content': content,
                        'accumulated': accumulated_content,
                        'chunk_index': chunk_count,
                        'stage': 'generating',
                        'timestamp': time.time()
                    }
            
            # Yield completion
            yield {
                'type': 'complete',
                'content': accumulated_content,
                'stage': 'generating',
                'metadata': {
                    'chunks': chunk_count,
                    'service': 'gemini',
                    'total_length': len(accumulated_content)
                }
            }
            
            logger.info(f"âœ… Gemini æµå¼è°ƒç”¨å®Œæˆï¼Œå…± {chunk_count} ä¸ªåˆ†å—ï¼Œæ€»é•¿åº¦: {len(accumulated_content)}")
            
        except Exception as e:
            logger.error(f"âŒ Gemini æµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise e
    
    async def _stream_doubao(self, prompt: str) -> AsyncIterator[Dict]:
        """Stream content from Doubao API (placeholder)"""
        logger.warning("âš ï¸ è±†åŒ…å¤§æ¨¡å‹æµå¼ API è°ƒç”¨å°šæœªå®Œå…¨å®ç°")
        
        # For now, simulate streaming with delays
        test_content = "ã€è±†åŒ…å¤§æ¨¡å‹æµå¼å†…å®¹å ä½ç¬¦ã€‘\n\n"
        test_content += "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„æµå¼å“åº”ï¼Œç”¨äºæµ‹è¯•ç³»ç»ŸåŠŸèƒ½ã€‚\n\n"
        test_content += "å®é™…å®ç°æ—¶ï¼Œè¿™é‡Œå°†åŒ…å«è±†åŒ…å¤§æ¨¡å‹ç”Ÿæˆçš„çœŸå®å†…å®¹ã€‚\n\n"
        
        accumulated_content = ""
        
        # Simulate streaming chunks
        for i, char in enumerate(test_content):
            accumulated_content += char
            
            yield {
                'type': 'chunk',
                'content': char,
                'accumulated': accumulated_content,
                'chunk_index': i + 1,
                'stage': 'generating'
            }
            
            # Small delay to simulate streaming
            await asyncio.sleep(0.01)
        
        # Yield completion
        yield {
            'type': 'complete',
            'content': accumulated_content,
            'stage': 'generating',
            'metadata': {
                'chunks': len(test_content),
                'service': 'doubao'
            }
        }
    
    async def _generate_summaries_streaming(self, full_content: str, city: str, industry: str) -> AsyncIterator[Dict]:
        """Generate summaries with streaming support"""
        
        # Chinese summary
        yield {
            'type': 'start',
            'stage': 'summary_zh',
            'message': 'æ­£åœ¨ç”Ÿæˆä¸­æ–‡æ‰§è¡Œæ‘˜è¦...'
        }
        
        summary_prompt_zh = f"""è¯·åŸºäºä»¥ä¸‹å®Œæ•´çš„äº§ä¸šåˆ†ææŠ¥å‘Šï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ‰§è¡Œæ‘˜è¦ï¼ˆExecutive Summaryï¼‰ï¼Œ
é•¿åº¦æ§åˆ¶åœ¨300-500å­—ï¼ŒåŒ…å«ï¼š
1. æ ¸å¿ƒå‘ç°ï¼ˆ2-3ç‚¹ï¼‰
2. å…³é”®æ•°æ®æŒ‡æ ‡ï¼ˆ2-3ä¸ªï¼‰
3. ä¸»è¦å»ºè®®ï¼ˆ2-3æ¡ï¼‰

æŠ¥å‘Šå†…å®¹ï¼š
{full_content[:3000]}

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸éœ€è¦é¢å¤–çš„æ ¼å¼è¯´æ˜ã€‚"""
        
        accumulated_summary_zh = ""
        
        if self.current_service == APIService.KIMI:
            async for chunk in self._stream_kimi(summary_prompt_zh):
                if chunk['type'] == 'chunk':
                    accumulated_summary_zh += chunk['content']
                    yield {
                        'type': 'summary_chunk',
                        'content': chunk['content'],
                        'language': 'zh',
                        'accumulated': accumulated_summary_zh
                    }
        elif self.current_service == APIService.GEMINI:
            async for chunk in self._stream_gemini(summary_prompt_zh):
                if chunk['type'] == 'chunk':
                    accumulated_summary_zh += chunk['content']
                    yield {
                        'type': 'summary_chunk',
                        'content': chunk['content'],
                        'language': 'zh',
                        'accumulated': accumulated_summary_zh
                    }
        
        yield {
            'type': 'summary_complete',
            'content': accumulated_summary_zh,
            'language': 'zh'
        }
        
        # English summary
        yield {
            'type': 'start',
            'stage': 'summary_en',
            'message': 'æ­£åœ¨ç”Ÿæˆè‹±æ–‡æ‰§è¡Œæ‘˜è¦...'
        }
        
        summary_prompt_en = f"""Based on the following industrial analysis report, 
generate a concise Executive Summary in English (200-300 words) including:
1. Key findings (2-3 points)
2. Critical metrics (2-3 items)
3. Main recommendations (2-3 items)

Report content:
{full_content[:3000]}

Please output the summary directly without additional formatting instructions."""
        
        accumulated_summary_en = ""
        
        if self.current_service == APIService.KIMI:
            async for chunk in self._stream_kimi(summary_prompt_en):
                if chunk['type'] == 'chunk':
                    accumulated_summary_en += chunk['content']
                    yield {
                        'type': 'summary_chunk',
                        'content': chunk['content'],
                        'language': 'en',
                        'accumulated': accumulated_summary_en
                    }
        elif self.current_service == APIService.GEMINI:
            async for chunk in self._stream_gemini(summary_prompt_en):
                if chunk['type'] == 'chunk':
                    accumulated_summary_en += chunk['content']
                    yield {
                        'type': 'summary_chunk',
                        'content': chunk['content'],
                        'language': 'en',
                        'accumulated': accumulated_summary_en
                    }
        
        yield {
            'type': 'summary_complete',
            'content': accumulated_summary_en,
            'language': 'en'
        }
    
    async def _generate_swot_streaming(self, full_content: str, city: str, industry: str) -> AsyncIterator[Dict]:
        """Generate SWOT analysis with streaming support"""
        
        yield {
            'type': 'start',
            'stage': 'swot',
            'message': 'æ­£åœ¨ç”Ÿæˆ SWOT åˆ†æ...'
        }
        
        swot_prompt = f"""è¯·åŸºäºä»¥ä¸‹äº§ä¸šåˆ†ææŠ¥å‘Šï¼Œç”Ÿæˆè¯¦ç»†çš„SWOTæˆ˜ç•¥åˆ†æã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªç»´åº¦ï¼ˆä¼˜åŠ¿ã€åŠ£åŠ¿ã€æœºé‡ã€å¨èƒï¼‰è‡³å°‘åˆ—å‡º4-6ä¸ªè¦ç‚¹
2. è¦ç‚¹è¦å…·ä½“ã€å¯æ“ä½œã€æœ‰æ´å¯ŸåŠ›
3. ç»“åˆæŠ¥å‘Šä¸­çš„å…·ä½“æ•°æ®å’Œæ¡ˆä¾‹
4. ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º

è¾“å‡ºæ ¼å¼ï¼š
{{
    "strengths": ["ä¼˜åŠ¿1ï¼šå…·ä½“æè¿°...", "ä¼˜åŠ¿2ï¼šå…·ä½“æè¿°..."],
    "weaknesses": ["åŠ£åŠ¿1ï¼šå…·ä½“æè¿°...", "åŠ£åŠ¿2ï¼šå…·ä½“æè¿°..."],
    "opportunities": ["æœºé‡1ï¼šå…·ä½“æè¿°...", "æœºé‡2ï¼šå…·ä½“æè¿°..."],
    "threats": ["å¨èƒ1ï¼šå…·ä½“æè¿°...", "å¨èƒ2ï¼šå…·ä½“æè¿°..."]
}}

æŠ¥å‘Šå†…å®¹ï¼š
{full_content[:4000]}

è¯·åªè¾“å‡ºJSONæ ¼å¼çš„å†…å®¹ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"""
        
        accumulated_swot = ""
        
        if self.current_service == APIService.KIMI:
            async for chunk in self._stream_kimi(swot_prompt):
                if chunk['type'] == 'chunk':
                    accumulated_swot += chunk['content']
                    yield {
                        'type': 'swot_chunk',
                        'content': chunk['content'],
                        'accumulated': accumulated_swot
                    }
        elif self.current_service == APIService.GEMINI:
            async for chunk in self._stream_gemini(swot_prompt):
                if chunk['type'] == 'chunk':
                    accumulated_swot += chunk['content']
                    yield {
                        'type': 'swot_chunk',
                        'content': chunk['content'],
                        'accumulated': accumulated_swot
                    }
        
        yield {
            'type': 'swot_complete',
            'content': accumulated_swot
        }
    
    def _prepare_prompt(self, city: str, industry: str, additional_context: str) -> str:
        """Prepare the prompt by replacing placeholders"""
        template_path = Path('industry_analysis_llm_prompt.md')
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                prompt_template = f.read()
        except Exception as e:
            logger.error(f"Error loading prompt template: {e}")
            prompt_template = self._get_default_prompt_template()
        
        prompt = prompt_template
        prompt = prompt.replace('[ç›®æ ‡åŸå¸‚]', city)
        prompt = prompt.replace('[ç›®æ ‡è¡Œä¸š]', industry)
        
        if additional_context:
            prompt += f"\n\nè¡¥å……ä¿¡æ¯å’Œè¦æ±‚ï¼š\n{additional_context}"
        
        prompt += "\n\nè¯·æŒ‰ç…§ä¸Šè¿°æ¡†æ¶ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†ã€ä¸“ä¸šçš„äº§ä¸šåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šåº”åŒ…å«å…·ä½“çš„æ•°æ®ã€æ¡ˆä¾‹å’Œæ´å¯Ÿã€‚"
        
        return prompt
    
    def _get_default_prompt_template(self) -> str:
        """Get default prompt template if file is not available"""
        return """è¯·ç”Ÿæˆä¸€ä»½å…³äº[ç›®æ ‡åŸå¸‚] [ç›®æ ‡è¡Œä¸š]çš„äº§ä¸šåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

1. æ‰§è¡Œæ‘˜è¦
2. äº§ä¸šæ¦‚è§ˆä¸æ ¸å¿ƒæ•°æ®
3. æ”¿ç­–ç¯å¢ƒåˆ†æ
4. äº§ä¸šç”Ÿæ€ä¸å…³é”®å‚ä¸è€…
5. äº§ä¸šé“¾åˆ†æ
6. AIèåˆæ½œåŠ›åˆ†æ
7. ç»“è®ºä¸å»ºè®®

è¯·æä¾›è¯¦ç»†ã€ä¸“ä¸šçš„åˆ†æå†…å®¹ã€‚"""
    
    def _parse_report_sections(self, content: str) -> Dict:
        """Parse the generated report into structured sections"""
        sections = {}
        
        section_markers = [
            ('executive_summary', ['1. æ‰§è¡Œæ‘˜è¦', 'Executive Summary']),
            ('industry_overview', ['2. äº§ä¸šæ¦‚è§ˆ', 'äº§ä¸šæ¦‚è§ˆä¸æ ¸å¿ƒæ•°æ®']),
            ('policy_landscape', ['3. æ”¿ç­–ç¯å¢ƒ', 'Policy Landscape']),
            ('ecosystem', ['4. äº§ä¸šç”Ÿæ€', 'äº§ä¸šç”Ÿæ€ä¸å…³é”®å‚ä¸è€…']),
            ('value_chain', ['5. äº§ä¸šé“¾åˆ†æ', 'Value Chain Analysis']),
            ('ai_integration', ['6. AIèåˆæ½œåŠ›', 'AI Integration Potential']),
            ('conclusion', ['7. ç»“è®º', 'Conclusion', 'æˆ˜ç•¥å»ºè®®'])
        ]
        
        lines = content.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            found_section = False
            for section_key, markers in section_markers:
                if any(marker in line for marker in markers):
                    if current_section:
                        sections[current_section] = '\n'.join(current_content).strip()
                    current_section = section_key
                    current_content = []
                    found_section = True
                    break
            
            if not found_section and current_section:
                current_content.append(line)
        
        if current_section:
            sections[current_section] = '\n'.join(current_content).strip()
        
        if not sections:
            sections['full_report'] = content
        
        return sections


# Import asyncio for async functionality
import asyncio


# Convenience function for synchronous streaming
def generate_report_streaming_sync(city: str, industry: str, 
                                 additional_context: str = "",
                                 llm_service: str = 'kimi',
                                 enable_fallback: bool = True) -> Iterator[Dict]:
    """Synchronous wrapper for streaming report generation"""
    
    async def _async_generator():
        generator = StreamingLLMReportGenerator(
            llm_service=llm_service,
            enable_fallback=enable_fallback
        )
        async for chunk in generator.generate_report_streaming(city, industry, additional_context):
            yield chunk
    
    # Run async generator in sync context
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        async_gen = _async_generator()
        while True:
            try:
                chunk = loop.run_until_complete(async_gen.__anext__())
                yield chunk
            except StopAsyncIteration:
                break
    finally:
        loop.close()


# Global streaming generator instance
streaming_generator = StreamingLLMReportGenerator()