#!/usr/bin/env python3
"""
LLM Report Generator using Kimi API (Moonshot AI)
Generates comprehensive regional industrial analysis reports.
"""

import os
import json
import logging
import time
from pathlib import Path
from typing import Dict, Optional, List
from openai import OpenAI
import google.generativeai as genai
from src.utils.api_error_handler import api_error_handler, handle_api_error, APIError, APIService

logger = logging.getLogger(__name__)


class LLMReportGenerator:
    """Generates industrial analysis reports using various LLM services with comprehensive error handling."""
    
    def __init__(self, config_path='config.json', llm_service: str = 'kimi', enable_fallback=True):
        """Initialize the LLM report generator.
        
        Args:
            config_path: Path to configuration file containing API keys
            llm_service: The LLM service to use ('kimi', 'gemini', or 'doubao')
        """
        logger.info("="*60)
        logger.info(f"åˆå§‹åŒ– {llm_service.upper()} LLM æŠ¥å‘Šç”Ÿæˆå™¨")
        logger.info("="*60)
        
        self.config = self._load_config(config_path)
        self.llm_service = llm_service
        self.enable_fallback = enable_fallback
        self.api_error_handler = api_error_handler
        self.available_services = self._detect_available_services()
        self.current_service = self._get_service_enum(llm_service)
        import threading
        self._client_lock = threading.Lock()
        self.usage_metrics = { 'kimi': 0, 'gemini': 0, 'doubao': 0 }
        
        if self.llm_service == 'kimi':
            api_keys_cfg = self.config.get('api_keys', {})
            self.api_key = (
                api_keys_cfg.get('kimi')
                or api_keys_cfg.get('kimi_api_key')
                or os.environ.get('KIMI_API_KEY')
                or os.environ.get('MOONSHOT_API_KEY')
            )
            if not self.api_key:
                logger.error("âŒ Kimi API Key æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–ç¯å¢ƒå˜é‡")
                raise ValueError("Kimi API key not found")
            with self._client_lock:
                self.client = OpenAI(
                    api_key=self.api_key,
                    base_url="https://api.moonshot.cn/v1"
                )
            self.model_name = os.environ.get('KIMI_MODEL', "moonshot-v1-128k")
            self.temperature = float(os.environ.get('KIMI_TEMPERATURE', 0.7))
            self.max_tokens = int(os.environ.get('KIMI_MAX_TOKENS', 8000))
            logger.info("âœ“ Kimi API é…ç½®æˆåŠŸ")

        elif self.llm_service == 'gemini':
            api_keys_cfg = self.config.get('api_keys', {})
            self.api_key = (
                api_keys_cfg.get('google_gemini')
                or api_keys_cfg.get('google_gemini_api_key')
                or os.environ.get('GOOGLE_GEMINI_API_KEY')
            )
            if not self.api_key:
                logger.error("âŒ Gemini API Key æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–ç¯å¢ƒå˜é‡")
                raise ValueError("Gemini API key not found")
            genai.configure(api_key=self.api_key)
            with self._client_lock:
                self.client = genai.GenerativeModel('gemini-1.5-pro-latest')
            self.model_name = "gemini-1.5-pro-latest"
            self.temperature = 0.7
            self.max_tokens = 8000
            logger.info("âœ“ Gemini API é…ç½®æˆåŠŸ")
            
        elif self.llm_service == 'doubao':
            self.api_key = self.config.get('api_keys', {}).get('doubao_api_key')
            if not self.api_key:
                logger.error("âŒ Doubao API Key æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥ config.json")
                raise ValueError("Doubao API key not found in config")
            
            logger.info(f"âœ“ Doubao API Key å·²åŠ è½½ (å‰10ä½): {self.api_key[:10]}...")
            # è±†åŒ…å¤§æ¨¡å‹é›†æˆé€»è¾‘å°†åœ¨è¿™é‡Œå®ç°
            self.client = None  # å ä½ç¬¦ï¼Œå®é™…å®ç°æ—¶éœ€è¦æ›¿æ¢
            self.model_name = "doubao-pro"  # å ä½ç¬¦ï¼Œå®é™…å®ç°æ—¶éœ€è¦æ›¿æ¢
            self.temperature = 0.7
            self.max_tokens = 8000
            logger.info("âœ“ Doubao API é…ç½®æˆåŠŸ")
        else:
            raise ValueError(f"Unsupported LLM service: {self.llm_service}")
        
        # åŠ è½½æç¤ºè¯æ¨¡æ¿
        logger.info("åŠ è½½æç¤ºè¯æ¨¡æ¿...")
        self.prompt_template = self._load_prompt_template()
        if self.prompt_template:
            logger.info(f"âœ“ æç¤ºè¯æ¨¡æ¿å·²åŠ è½½ (é•¿åº¦: {len(self.prompt_template)} å­—ç¬¦)")
        else:
            logger.warning("âš ï¸ æç¤ºè¯æ¨¡æ¿ä¸ºç©ºï¼Œå°†ä½¿ç”¨é»˜è®¤æ ¼å¼")
        
        logger.info("="*60)
        logger.info(f"âœ… {self.llm_service.upper()} LLM æŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info("="*60)
    
    def _get_service_enum(self, service_name: str) -> APIService:
        """Convert service name to enum"""
        try:
            return APIService(service_name.lower())
        except ValueError:
            logger.warning(f"æœªçŸ¥æœåŠ¡ç±»å‹: {service_name}ï¼Œé»˜è®¤ä½¿ç”¨ Kimi")
            return APIService.KIMI
    
    def _detect_available_services(self) -> List[APIService]:
        """Detect which services have valid API keys configured"""
        available = []
        
        api_keys_cfg = self.config.get('api_keys', {})
        # Check Kimi
        if api_keys_cfg.get('kimi') or api_keys_cfg.get('kimi_api_key') or os.environ.get('KIMI_API_KEY') or os.environ.get('MOONSHOT_API_KEY'):
            available.append(APIService.KIMI)
        
        # Check Gemini
        if api_keys_cfg.get('google_gemini') or api_keys_cfg.get('google_gemini_api_key') or os.environ.get('GOOGLE_GEMINI_API_KEY'):
            available.append(APIService.GEMINI)
        
        # Check Doubao
        if self.config.get('api_keys', {}).get('doubao_api_key'):
            available.append(APIService.DOUBAO)
        
        logger.info(f"âœ… æ£€æµ‹åˆ°å¯ç”¨æœåŠ¡: {[s.value for s in available]}")
        return available
    
    def _load_config(self, config_path: str) -> Dict:
        """Load configuration from JSON file."""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading config: {e}")
            return {}
    
    def _load_prompt_template(self) -> str:
        """Load the prompt template from file."""
        template_path = Path('industry_analysis_llm_prompt.md')
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"Error loading prompt template: {e}")
            return ""
    
    def generate_report(self, city: str, industry: str, 
                       additional_context: str = "", max_fallback_attempts: int = 2) -> Dict:
        """Generate a comprehensive industrial analysis report with intelligent error handling and fallback.
        
        Args:
            city: Target city name (e.g., "æˆéƒ½", "é‡åº†")
            industry: Target industry name (e.g., "äººå·¥æ™ºèƒ½", "æ±½è½¦äº§ä¸š")
            additional_context: Additional context or requirements
            max_fallback_attempts: Maximum number of fallback attempts to other services
        
        Returns:
            Dictionary containing the generated report and metadata, with error information if failed
        """
        try:
            # å‡†å¤‡æç¤ºè¯
            prompt = self._prepare_prompt(city, industry, additional_context)
            
            # ç”ŸæˆæŠ¥å‘Š
            logger.info("="*60)
            logger.info(f"ğŸš€ å¼€å§‹ç”ŸæˆæŠ¥å‘Š: {city} - {industry}")
            logger.info(f"ğŸŒ é¦–é€‰æœåŠ¡: {self.llm_service.upper()}")
            logger.info(f"ğŸ”„ å¯ç”¨å›é€€æœºåˆ¶: {self.enable_fallback}")
            logger.info("="*60)
            logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # å°è¯•ç”ŸæˆæŠ¥å‘Šï¼Œæ”¯æŒæœåŠ¡å›é€€
            return self._generate_report_with_fallback(
                city, industry, prompt, max_fallback_attempts
            )
            
        except Exception as e:
            logger.error("="*60)
            logger.error(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
            logger.error("="*60)
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            
            import traceback
            logger.error("å®Œæ•´å †æ ˆè·Ÿè¸ª:")
            logger.error(traceback.format_exc())
            
            # ä½¿ç”¨é”™è¯¯å¤„ç†å™¨åˆ†æé”™è¯¯
            api_error = handle_api_error(e, self.llm_service, "æŠ¥å‘Šç”Ÿæˆ")
            
            return {
                'success': False,
                'error': str(e),
                'api_error': {
                    'type': api_error.error_type.value,
                    'service': api_error.service.value,
                    'user_message': api_error.user_friendly_message,
                    'suggested_action': api_error.suggested_action,
                    'retry_after': api_error.retry_after
                },
                'city': city,
                'industry': industry,
                'failed_service': self.llm_service
            }
    
    def _call_kimi_api(self, prompt: str, start_time: float) -> Dict:
        """Call Kimi API"""
        completion = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äº§ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿æ’°å†™æ·±åº¦çš„åŒºåŸŸäº§ä¸šåˆ†ææŠ¥å‘Šã€‚è¯·åŸºäºç”¨æˆ·æä¾›çš„æ¡†æ¶å’Œè¦æ±‚ï¼Œç”Ÿæˆè¯¦å®ã€ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        
        report_content = completion.choices[0].message.content
        prompt_tokens = completion.usage.prompt_tokens
        completion_tokens = completion.usage.completion_tokens
        total_tokens = completion.usage.total_tokens
        
        elapsed = time.time() - start_time
        logger.info(f"âœ… Kimi API è°ƒç”¨æˆåŠŸï¼è€—æ—¶: {elapsed:.2f} ç§’")
        
        return {
            'success': True,
            'content': report_content,
            'tokens': {
                'prompt': prompt_tokens,
                'completion': completion_tokens,
                'total': total_tokens
            }
        }
    def _generate_report_with_fallback(self, city: str, industry: str, prompt: str, 
                                       max_fallback_attempts: int) -> Dict:
        """Generate report with intelligent fallback between services"""
        
        services_to_try = [self.current_service] + [
            s for s in self.available_services 
            if s != self.current_service
        ]
        
        last_error = None
        attempted_services = []
        
        for i, service in enumerate(services_to_try[:max_fallback_attempts + 1]):
            service_name = service.value
            attempted_services.append(service_name)
            
            logger.info(f"\nğŸ”„ å°è¯•æœåŠ¡ {i+1}/{len(services_to_try)}: {service_name.upper()}")
            
            try:
                # ä¸´æ—¶åˆ‡æ¢åˆ°ç›®æ ‡æœåŠ¡
                original_service = self.llm_service
                self.llm_service = service_name
                self.current_service = service
                
                # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯
                self._reinitialize_client(service_name)
                
                # ç”ŸæˆæŠ¥å‘Š
                result = self._call_api_with_retry(service_name, prompt)
                
                # æ¢å¤åŸå§‹æœåŠ¡
                self.llm_service = original_service
                self.current_service = self._get_service_enum(original_service)
                
                if result and result.get('success'):
                    logger.info(f"âœ… ä½¿ç”¨ {service_name.upper()} æˆåŠŸç”ŸæˆæŠ¥å‘Šï¼")
                    
                    # ä½¿ç”¨è¿”å›çš„å†…å®¹ç”Ÿæˆå®Œæ•´æŠ¥å‘Šç»“æ„
                    report_content = result['content']
                    tokens = result['tokens']
                    
                    # è§£ææŠ¥å‘Šç« èŠ‚
                    logger.info("ğŸ” è§£ææŠ¥å‘Šç« èŠ‚...")
                    sections = self._parse_report_sections(report_content)
                    logger.info(f"âœ“ è§£æå®Œæˆï¼Œå…± {len(sections)} ä¸ªç« èŠ‚: {list(sections.keys())}")
                    
                    logger.info("="*60)
                    logger.info("âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
                    logger.info("="*60)
                    
                    return {
                        'success': True,
                        'city': city,
                        'industry': industry,
                        'full_content': report_content,
                        'sections': sections,
                        'metadata': {
                            'generated_at': None,
                            'model': self.model_name,
                            'provider': service_name,
                            'prompt_version': '1.0',
                            'tokens': tokens
                        },
                        'used_service': service_name,
                        'attempted_services': attempted_services
                    }
                
            except Exception as e:
                logger.warning(f"âŒ {service_name.upper()} æœåŠ¡å¤±è´¥: {str(e)}")
                last_error = e
                
                # ä½¿ç”¨é”™è¯¯å¤„ç†å™¨åˆ†æé”™è¯¯
                api_error = handle_api_error(e, service_name, f"æœåŠ¡å›é€€å°è¯• {i+1}")
                
                # å¦‚æœæ˜¯é…é¢è¶…é™ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæœåŠ¡
                if api_error.error_type.value == 'quota_exceeded':
                    logger.info(f"â¡ï¸  ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæœåŠ¡...")
                    continue
                
                # å¦‚æœæ˜¯è¿æ¥é—®é¢˜ï¼Œä¹Ÿå¯ä»¥å°è¯•å…¶ä»–æœåŠ¡
                elif api_error_handler.is_connection_issue(service, e):
                    logger.info(f"â¡ï¸  è¿æ¥é—®é¢˜ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæœåŠ¡...")
                    continue
                
                # æ¢å¤åŸå§‹æœåŠ¡
                self.llm_service = original_service
                self.current_service = self._get_service_enum(original_service)
        
        # æ‰€æœ‰æœåŠ¡éƒ½å¤±è´¥äº†
        logger.error(f"âŒ æ‰€æœ‰å¯ç”¨æœåŠ¡éƒ½å¤±è´¥ï¼Œå·²å°è¯•: {attempted_services}")
        if last_error:
            raise last_error
        else:
            raise Exception("æ‰€æœ‰ AI æœåŠ¡éƒ½æ— æ³•ç”ŸæˆæŠ¥å‘Š")
    
    def _reinitialize_client(self, service_name: str):
        """Reinitialize the API client for the specified service"""
        logger.info(f"ğŸ”§ é‡æ–°åˆå§‹åŒ– {service_name.upper()} å®¢æˆ·ç«¯...")
        
        if service_name == 'kimi':
            api_keys_cfg = self.config.get('api_keys', {})
            self.api_key = (
                api_keys_cfg.get('kimi')
                or api_keys_cfg.get('kimi_api_key')
                or os.environ.get('KIMI_API_KEY')
                or os.environ.get('MOONSHOT_API_KEY')
            )
            if not self.api_key:
                raise ValueError("Kimi API key not found in config")

            self.client = OpenAI(
                api_key=self.api_key,
                base_url="https://api.moonshot.cn/v1"
            )
            self.model_name = "moonshot-v1-128k"

        elif service_name == 'gemini':
            api_keys_cfg = self.config.get('api_keys', {})
            self.api_key = (
                api_keys_cfg.get('google_gemini')
                or api_keys_cfg.get('google_gemini_api_key')
                or os.environ.get('GOOGLE_GEMINI_API_KEY')
            )
            if not self.api_key:
                raise ValueError("Gemini API key not found in config")

            genai.configure(api_key=self.api_key)
            self.client = genai.GenerativeModel('gemini-pro')
            self.model_name = "gemini-pro"

        elif service_name == 'doubao':
            self.api_key = self.config.get('api_keys', {}).get('doubao_api_key')
            if not self.api_key:
                raise ValueError("Doubao API key not found in config")

            # è±†åŒ…å¤§æ¨¡å‹å®¢æˆ·ç«¯åˆå§‹åŒ–é€»è¾‘
            self.client = None  # å ä½ç¬¦
            self.model_name = "doubao-pro"
    
    def _call_api_with_retry(self, service_name: str, prompt: str) -> Dict:
        """Call API with intelligent retry logic"""
        max_retries = 3
        base_delay = 2
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ“¡ API è°ƒç”¨å°è¯• {attempt + 1}/{max_retries}...")
                start_time = time.time()
                
                # Key rotation check and reinit if necessary
                self._check_key_rotation_and_reinit(service_name)
                if service_name == 'kimi':
                    return self._call_kimi_api(prompt, start_time)
                elif service_name == 'gemini':
                    return self._call_gemini_api(prompt, start_time)
                elif service_name == 'doubao':
                    return self._call_doubao_api(prompt, start_time)
                
            except Exception as e:
                elapsed = time.time() - start_time
                logger.error(f"âŒ API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}ï¼Œè€—æ—¶ {elapsed:.2f}ç§’)")
                
                # ä½¿ç”¨é”™è¯¯å¤„ç†å™¨åˆ†æé”™è¯¯
                api_error = handle_api_error(e, service_name, f"API è°ƒç”¨å°è¯• {attempt + 1}")
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç«‹å³é‡è¯•
                if not api_error_handler.should_retry_immediately(api_error):
                    logger.warning(f"â¹ï¸  é”™è¯¯ç±»å‹ä¸å»ºè®®é‡è¯•: {api_error.error_type.value}")
                    raise e
                
                if attempt < max_retries - 1:
                    delay = base_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    retry_after = api_error.retry_after or delay
                    logger.warning(f"â³ ç­‰å¾… {retry_after} ç§’åé‡è¯•...")
                    time.sleep(retry_after)
                else:
                    logger.error("ğŸ’¥ æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œæ”¾å¼ƒè¯·æ±‚")
                    raise
    
    def _call_kimi_api(self, prompt: str, start_time: float) -> Dict:
        """Call Kimi API"""
        with self._client_lock:
            completion = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äº§ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿æ’°å†™æ·±åº¦çš„åŒºåŸŸäº§ä¸šåˆ†ææŠ¥å‘Šã€‚è¯·åŸºäºç”¨æˆ·æä¾›çš„æ¡†æ¶å’Œè¦æ±‚ï¼Œç”Ÿæˆè¯¦å®ã€ä¸“ä¸šçš„åˆ†ææŠ¥å‘Šã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )
        
        report_content = completion.choices[0].message.content
        prompt_tokens = completion.usage.prompt_tokens
        completion_tokens = completion.usage.completion_tokens
        total_tokens = completion.usage.total_tokens
        
        elapsed = time.time() - start_time
        logger.info(f"âœ… Kimi API è°ƒç”¨æˆåŠŸï¼è€—æ—¶: {elapsed:.2f} ç§’")
        self.usage_metrics['kimi'] += 1
        
        return {
            'success': True,
            'content': report_content,
            'tokens': {
                'prompt': prompt_tokens,
                'completion': completion_tokens,
                'total': total_tokens
            }
        }
    
    def _call_gemini_api(self, prompt: str, start_time: float) -> Dict:
        """Call Gemini API"""
        with self._client_lock:
            response = self.client.generate_content(
            prompt,
            generation_config={
                'temperature': self.temperature,
                'max_output_tokens': self.max_tokens
            }
        )
        
        report_content = response.text
        elapsed = time.time() - start_time
        logger.info(f"âœ… Gemini API è°ƒç”¨æˆåŠŸï¼è€—æ—¶: {elapsed:.2f} ç§’")
        self.usage_metrics['gemini'] += 1
        
        return {
            'success': True,
            'content': report_content,
            'tokens': {
                'prompt': 0,  # Gemini doesn't provide detailed token usage
                'completion': 0,
                'total': 0
            }
        }
    
    def _call_doubao_api(self, prompt: str, start_time: float) -> Dict:
        """Call Doubao API (placeholder implementation)"""
        logger.warning("âš ï¸ è±†åŒ…å¤§æ¨¡å‹ API è°ƒç”¨å°šæœªå®Œå…¨å®ç°")
        
        # æ¨¡æ‹Ÿå“åº”æˆ–æŠ›å‡ºç‰¹å®šé”™è¯¯
        raise NotImplementedError("è±†åŒ…å¤§æ¨¡å‹ API é›†æˆå°šæœªå®Œæˆ")
            
    def _parse_report_sections(self, content: str) -> Dict:
        """Parse the generated report into structured sections."""
        sections = {}
        
        section_markers = [
            ('executive_summary', ['1. æ‰§è¡Œæ‘˜è¦', 'Executive Summary']),
            ('industry_overview', ['2. äº§ä¸šæ¦‚è§ˆ', 'äº§ä¸šæ¦‚è§ˆä¸æ ¸å¿ƒæ•°æ®']),
            ('policy_landscape', ['3. æ”¿ç­–ç¯å¢ƒ', 'Policy Landscape']),
            ('ecosystem', ['4. äº§ä¸šç”Ÿæ€', 'äº§ä¸šç”Ÿæ€ä¸å…³é”®å‚ä¸è€…']),
            ('value_chain', ['5. äº§ä¸šé“¾åˆ†æ', 'Value Chain Analysis']),
            ('ai_integration', ['6. AIèåˆæ½œåŠ›', 'AI Integration Potential']),
            ('conclusion', ['7. ç»“è®º', 'Conclusion', 'æˆ˜ç•¥å»ºè®®'])
        ]
        
        lines = content.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            found_section = False
            for section_key, markers in section_markers:
                if any(marker in line for marker in markers):
                    if current_section:
                        sections[current_section] = '\n'.join(current_content).strip()
                    current_section = section_key
                    current_content = []
                    found_section = True
                    break
            
            if not found_section and current_section:
                current_content.append(line)
        
        if current_section:
            sections[current_section] = '\n'.join(current_content).strip()
        
        if not sections:
            sections['full_report'] = content
        
        return sections
            
    def _prepare_prompt(self, city: str, industry: str, 
                       additional_context: str) -> str:
        """Prepare the prompt by replacing placeholders."""
        prompt = self.prompt_template
        prompt = prompt.replace('[ç›®æ ‡åŸå¸‚]', city)
        prompt = prompt.replace('[ç›®æ ‡è¡Œä¸š]', industry)
        
        if additional_context:
            prompt += f"\n\nè¡¥å……ä¿¡æ¯å’Œè¦æ±‚ï¼š\n{additional_context}"
        
        prompt += "\n\nè¯·æŒ‰ç…§ä¸Šè¿°æ¡†æ¶ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†ã€ä¸“ä¸šçš„äº§ä¸šåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šåº”åŒ…å«å…·ä½“çš„æ•°æ®ã€æ¡ˆä¾‹å’Œæ´å¯Ÿã€‚"
        
        return prompt
    
    def _prepare_prompt(self, city: str, industry: str, 
                       additional_context: str) -> str:
        """Prepare the prompt by replacing placeholders."""
        prompt = self.prompt_template
        prompt = prompt.replace('[ç›®æ ‡åŸå¸‚]', city)
        prompt = prompt.replace('[ç›®æ ‡è¡Œä¸š]', industry)
        
        if additional_context:
            prompt += f"\n\nè¡¥å……ä¿¡æ¯å’Œè¦æ±‚ï¼š\n{additional_context}"
        
        prompt += "\n\nè¯·æŒ‰ç…§ä¸Šè¿°æ¡†æ¶ï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†ã€ä¸“ä¸šçš„äº§ä¸šåˆ†ææŠ¥å‘Šã€‚æŠ¥å‘Šåº”åŒ…å«å…·ä½“çš„æ•°æ®ã€æ¡ˆä¾‹å’Œæ´å¯Ÿã€‚"
        
        return prompt
    
    def _parse_report_sections(self, content: str) -> Dict:
        """Parse the generated report into structured sections."""
        sections = {}
        
        section_markers = [
            ('executive_summary', ['1. æ‰§è¡Œæ‘˜è¦', 'Executive Summary']),
            ('industry_overview', ['2. äº§ä¸šæ¦‚è§ˆ', 'äº§ä¸šæ¦‚è§ˆä¸æ ¸å¿ƒæ•°æ®']),
            ('policy_landscape', ['3. æ”¿ç­–ç¯å¢ƒ', 'Policy Landscape']),
            ('ecosystem', ['4. äº§ä¸šç”Ÿæ€', 'äº§ä¸šç”Ÿæ€ä¸å…³é”®å‚ä¸è€…']),
            ('value_chain', ['5. äº§ä¸šé“¾åˆ†æ', 'Value Chain Analysis']),
            ('ai_integration', ['6. AIèåˆæ½œåŠ›', 'AI Integration Potential']),
            ('conclusion', ['7. ç»“è®º', 'Conclusion', 'æˆ˜ç•¥å»ºè®®'])
        ]
        
        lines = content.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            found_section = False
            for section_key, markers in section_markers:
                if any(marker in line for marker in markers):
                    if current_section:
                        sections[current_section] = '\n'.join(current_content).strip()
                    current_section = section_key
                    current_content = []
                    found_section = True
                    break
            
            if not found_section and current_section:
                current_content.append(line)
        
        if current_section:
            sections[current_section] = '\n'.join(current_content).strip()
        
        if not sections:
            sections['full_report'] = content
        
        return sections
    
    def stream_report_content(self, city: str, industry: str, additional_context: str = ""):
        """Stream main report content chunks from the LLM service."""
        import time
        try:
            prompt = self._prepare_prompt(city, industry, additional_context)
            if self.llm_service == 'kimi':
                stream = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äº§ä¸šåˆ†æå¸ˆã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=self.max_tokens,
                    stream=True
                )
                for chunk in stream:
                    delta = getattr(chunk.choices[0], 'delta', None)
                    if delta and getattr(delta, 'content', None):
                        yield delta.content
            elif self.llm_service == 'gemini':
                response = self.client.generate_content(prompt, stream=True)
                for chunk in response:
                    if hasattr(chunk, 'text') and chunk.text:
                        yield chunk.text
            else:
                # Fallback: no streaming support
                result = self.generate_report(city, industry, additional_context)
                yield result.get('full_content', '')
        except Exception as e:
            logger.error(f"Streaming failed: {e}")
            yield f"[æµå¼ç”Ÿæˆå‡ºé”™] {e}"

    def generate_summary(self, full_report: str, language: str = 'zh') -> str:
        """Generate a concise summary of the full report."""
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“ ç”Ÿæˆ{language.upper()}æ‘˜è¦...")
        logger.info(f"{'='*60}")
        
        try:
            if language == 'zh':
                summary_prompt = f"""è¯·åŸºäºä»¥ä¸‹å®Œæ•´çš„äº§ä¸šåˆ†ææŠ¥å‘Šï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ‰§è¡Œæ‘˜è¦ï¼ˆExecutive Summaryï¼‰ï¼Œ
é•¿åº¦æ§åˆ¶åœ¨300-500å­—ï¼ŒåŒ…å«ï¼š
1. æ ¸å¿ƒå‘ç°ï¼ˆ2-3ç‚¹ï¼‰
2. å…³é”®æ•°æ®æŒ‡æ ‡ï¼ˆ2-3ä¸ªï¼‰
3. ä¸»è¦å»ºè®®ï¼ˆ2-3æ¡ï¼‰

æŠ¥å‘Šå†…å®¹ï¼š
{full_report[:3000]}

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸éœ€è¦é¢å¤–çš„æ ¼å¼è¯´æ˜ã€‚"""
            else:
                summary_prompt = f"""Based on the following industrial analysis report, 
generate a concise Executive Summary in English (200-300 words) including:
1. Key findings (2-3 points)
2. Critical metrics (2-3 items)
3. Main recommendations (2-3 items)

Report content:
{full_report[:3000]}

Please output the summary directly without additional formatting instructions."""
            
            logger.info("ğŸŒ è°ƒç”¨ API ç”Ÿæˆæ‘˜è¦...")
            start_time = time.time()
            
            if self.llm_service == 'kimi':
                completion = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "user", "content": summary_prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=1000
                )
                summary_content = completion.choices[0].message.content
            elif self.llm_service == 'gemini':
                response = self.client.generate_content(summary_prompt)
                summary_content = response.text
            elif self.llm_service == 'doubao':
                # è°ƒç”¨è±†åŒ…å¤§æ¨¡å‹ API ç”Ÿæˆæ‘˜è¦ (å ä½ç¬¦å®ç°)
                logger.warning("âš ï¸ è±†åŒ…å¤§æ¨¡å‹æ‘˜è¦ç”ŸæˆåŠŸèƒ½å°šæœªå®Œå…¨å®ç°")
                summary_content = ""  # å ä½ç¬¦ï¼Œå®é™…å®ç°æ—¶éœ€è¦æ›¿æ¢

            elapsed = time.time() - start_time
            logger.info(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {elapsed:.2f} ç§’")
            logger.info(f"ğŸ“Š æ‘˜è¦é•¿åº¦: {len(summary_content)} å­—ç¬¦")
            return summary_content
        
        except Exception as e:
            logger.error(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {type(e).__name__} - {str(e)}")
            return "æ‘˜è¦ç”Ÿæˆå¤±è´¥" if language == 'zh' else "Summary generation failed"
    
    def generate_swot_analysis(self, full_report: str) -> Dict:
        """Generate SWOT analysis from the full report."""
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š ç”Ÿæˆ SWOT åˆ†æ...")
        logger.info(f"{'='*60}")
        
        try:
            swot_prompt = f"""è¯·åŸºäºä»¥ä¸‹äº§ä¸šåˆ†ææŠ¥å‘Šï¼Œç”Ÿæˆè¯¦ç»†çš„SWOTæˆ˜ç•¥åˆ†æã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªç»´åº¦ï¼ˆä¼˜åŠ¿ã€åŠ£åŠ¿ã€æœºé‡ã€å¨èƒï¼‰è‡³å°‘åˆ—å‡º4-6ä¸ªè¦ç‚¹
2. è¦ç‚¹è¦å…·ä½“ã€å¯æ“ä½œã€æœ‰æ´å¯ŸåŠ›
3. ç»“åˆæŠ¥å‘Šä¸­çš„å…·ä½“æ•°æ®å’Œæ¡ˆä¾‹
4. ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º

è¾“å‡ºæ ¼å¼ï¼š
{{
    "strengths": ["ä¼˜åŠ¿1ï¼šå…·ä½“æè¿°...", "ä¼˜åŠ¿2ï¼šå…·ä½“æè¿°...", "ä¼˜åŠ¿3ï¼šå…·ä½“æè¿°...", "ä¼˜åŠ¿4ï¼šå…·ä½“æè¿°..."],
    "weaknesses": ["åŠ£åŠ¿1ï¼šå…·ä½“æè¿°...", "åŠ£åŠ¿2ï¼šå…·ä½“æè¿°...", "åŠ£åŠ¿3ï¼šå…·ä½“æè¿°...", "åŠ£åŠ¿4ï¼šå…·ä½“æè¿°..."],
    "opportunities": ["æœºé‡1ï¼šå…·ä½“æè¿°...", "æœºé‡2ï¼šå…·ä½“æè¿°...", "æœºé‡3ï¼šå…·ä½“æè¿°...", "æœºé‡4ï¼šå…·ä½“æè¿°..."],
    "threats": ["å¨èƒ1ï¼šå…·ä½“æè¿°...", "å¨èƒ2ï¼šå…·ä½“æè¿°...", "å¨èƒ3ï¼šå…·ä½“æè¿°...", "å¨èƒ4ï¼šå…·ä½“æè¿°..."]
}}

æŠ¥å‘Šå†…å®¹ï¼š
{full_report[:4000]}

è¯·åªè¾“å‡ºJSONæ ¼å¼çš„å†…å®¹ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°æˆ–å…¶ä»–è¯´æ˜æ–‡å­—ã€‚"""
            
            logger.info("ğŸŒ è°ƒç”¨ API ç”Ÿæˆ SWOT...")
            start_time = time.time()
            
            if self.llm_service == 'kimi':
                completion = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æˆ˜ç•¥åˆ†æå¸ˆï¼Œæ“…é•¿è¿›è¡ŒSWOTåˆ†æã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ ‡è®°æˆ–é¢å¤–è¯´æ˜ã€‚"
                        },
                        {
                            "role": "user",
                            "content": swot_prompt
                        }
                    ],
                    temperature=0.7,
                    max_tokens=2000
                )
                response_text = completion.choices[0].message.content.strip()
            elif self.llm_service == 'gemini':
                response = self.client.generate_content(swot_prompt)
                response_text = response.text.strip()
            elif self.llm_service == 'doubao':
                # è°ƒç”¨è±†åŒ…å¤§æ¨¡å‹ API ç”Ÿæˆ SWOT åˆ†æ (å ä½ç¬¦å®ç°)
                logger.warning("âš ï¸ è±†åŒ…å¤§æ¨¡å‹ SWOT åˆ†æåŠŸèƒ½å°šæœªå®Œå…¨å®ç°")
                response_text = "{}"  # å ä½ç¬¦ï¼Œå®é™…å®ç°æ—¶éœ€è¦æ›¿æ¢

            elapsed = time.time() - start_time
            logger.info(f"âœ… SWOT ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {elapsed:.2f} ç§’")
            
            logger.info(f"ğŸ“„ åŸå§‹å“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
            logger.info(f"ğŸ“„ å“åº”é¢„è§ˆ: {response_text[:200]}...")
            
            import re
            response_text = re.sub(r'^```json\s*', '', response_text)
            response_text = re.sub(r'^```\s*', '', response_text)
            response_text = re.sub(r'\s*```$', '', response_text)
            response_text = response_text.strip()
            
            try:
                swot_data = json.loads(response_text)
                logger.info(f"âœ“ SWOT JSON è§£ææˆåŠŸ")
                logger.info(f"  - ä¼˜åŠ¿: {len(swot_data.get('strengths', []))} é¡¹")
                logger.info(f"  - åŠ£åŠ¿: {len(swot_data.get('weaknesses', []))} é¡¹")
                logger.info(f"  - æœºé‡: {len(swot_data.get('opportunities', []))} é¡¹")
                logger.info(f"  - å¨èƒ: {len(swot_data.get('threats', []))} é¡¹")
                
                if not any([swot_data.get('strengths'), swot_data.get('weaknesses'), 
                           swot_data.get('opportunities'), swot_data.get('threats')]):
                    logger.warning("âš ï¸ SWOT æ‰€æœ‰å­—æ®µä¸ºç©ºï¼Œä½¿ç”¨æ–‡æœ¬è§£æ")
                    return self._parse_swot_from_text(response_text)
                
                return swot_data
            except json.JSONDecodeError as je:
                logger.warning(f"âš ï¸ SWOT å“åº”ä¸æ˜¯æœ‰æ•ˆ JSON: {str(je)}")
                logger.info(f"å°è¯•è§£æçš„æ–‡æœ¬: {response_text[:500]}")
                return self._parse_swot_from_text(response_text)
        
        except Exception as e:
            logger.error(f"âŒ SWOT ç”Ÿæˆå¤±è´¥: {type(e).__name__} - {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            
            logger.warning("âš ï¸ APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬è§£æä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
            try:
                return self._extract_swot_from_full_report(full_report)
            except:
                return {
                    'strengths': [],
                    'weaknesses': [],
                    'opportunities': [],
                    'threats': []
                }
    
    def _parse_swot_from_text(self, text: str) -> Dict:
        """Parse SWOT analysis from plain text response."""
        logger.info("ğŸ“ è§£æ SWOT æ–‡æœ¬å†…å®¹...")
        logger.info(f"åŸå§‹å“åº” (å‰500å­—ç¬¦): {text[:500]}")
        
        swot = {
            'strengths': [],
            'weaknesses': [],
            'opportunities': [],
            'threats': []
        }
        
        import re
        
        text_cleaned = re.sub(r'```json\s*', '', text)
        text_cleaned = re.sub(r'```\s*', '', text_cleaned)
        
        try:
            swot_data = json.loads(text_cleaned.strip())
            logger.info("âœ“ æ¸…ç†åçš„æ–‡æœ¬æˆåŠŸè§£æä¸º JSON")
            return swot_data
        except json.JSONDecodeError:
            logger.info("âœ— JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æ¨¡å¼")
        
        lines = text.split('\n')
        current_category = None
        
        for line in lines:
            line = line.strip()
            
            if not line:
                continue
                
            if 'ä¼˜åŠ¿' in line or 'Strength' in line.lower() or '"strengths"' in line:
                current_category = 'strengths'
                logger.info(f"  ğŸ” æ‰¾åˆ°ä¼˜åŠ¿éƒ¨åˆ†")
            elif 'åŠ£åŠ¿' in line or 'å¼±ç‚¹' in line or 'Weakness' in line.lower() or '"weaknesses"' in line:
                current_category = 'weaknesses'
                logger.info(f"  ğŸ” æ‰¾åˆ°åŠ£åŠ¿éƒ¨åˆ†")
            elif 'æœºé‡' in line or 'æœºä¼š' in line or 'Opportunit' in line or '"opportunities"' in line:
                current_category = 'opportunities'
                logger.info(f"  ğŸ” æ‰¾åˆ°æœºé‡éƒ¨åˆ†")
            elif 'å¨èƒ' in line or 'Threat' in line.lower() or '"threats"' in line:
                current_category = 'threats'
                logger.info(f"  ğŸ” æ‰¾åˆ°å¨èƒéƒ¨åˆ†")
            elif current_category:
                if line.startswith('-') or line.startswith('â€¢') or line.startswith('*') or re.match(r'^\d+[\.\)ã€]', line):
                    item = re.sub(r'^[-â€¢*\d+\.\)ã€\s]+', '', line).strip()
                    item = item.strip('"\'ã€,ï¼Œ')
                    if item and len(item) > 2:
                        swot[current_category].append(item)
                        logger.info(f"    âœ“ æ·»åŠ é¡¹ç›®: {item[:50]}...")
        
        logger.info(f"ğŸ“Š è§£æç»“æœç»Ÿè®¡:")
        logger.info(f"  - ä¼˜åŠ¿: {len(swot['strengths'])} é¡¹")
        logger.info(f"  - åŠ£åŠ¿: {len(swot['weaknesses'])} é¡¹")
        logger.info(f"  - æœºé‡: {len(swot['opportunities'])} é¡¹")
        logger.info(f"  - å¨èƒ: {len(swot['threats'])} é¡¹")
        
        return swot
    
    def _extract_swot_from_full_report(self, full_report: str) -> Dict:
        """ä»å®Œæ•´æŠ¥å‘Šä¸­æå–SWOTä¿¡æ¯ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ"""
        logger.info("ğŸ“ ä»æŠ¥å‘Šå†…å®¹ä¸­æå–SWOT...")
        
        swot = {
            'strengths': [],
            'weaknesses': [],
            'opportunities': [],
            'threats': []
        }
        
        # ç®€å•çš„å…³é”®è¯æå–
        lines = full_report.split('\n')
        for line in lines:
            line_lower = line.lower()
            if any(word in line for word in ['ä¼˜åŠ¿', 'å¼ºé¡¹', 'ç«äº‰åŠ›']):
                if len(line.strip()) > 10 and not line.startswith('#'):
                    swot['strengths'].append(line.strip())
            elif any(word in line for word in ['åŠ£åŠ¿', 'ä¸è¶³', 'çŸ­æ¿', 'å¼±ç‚¹']):
                if len(line.strip()) > 10 and not line.startswith('#'):
                    swot['weaknesses'].append(line.strip())
            elif any(word in line for word in ['æœºé‡', 'æœºä¼š', 'æ½œåŠ›']):
                if len(line.strip()) > 10 and not line.startswith('#'):
                    swot['opportunities'].append(line.strip())
            elif any(word in line for word in ['å¨èƒ', 'é£é™©', 'æŒ‘æˆ˜']):
                if len(line.strip()) > 10 and not line.startswith('#'):
                    swot['threats'].append(line.strip())
        
        # é™åˆ¶æ¯ä¸ªç»´åº¦æœ€å¤š5æ¡
        for key in swot:
            swot[key] = swot[key][:5]
        
        logger.info(f"âœ“ ä»æŠ¥å‘Šæå–SWOT: ä¼˜åŠ¿{len(swot['strengths'])}, åŠ£åŠ¿{len(swot['weaknesses'])}, æœºé‡{len(swot['opportunities'])}, å¨èƒ{len(swot['threats'])}")
        return swot
    
    def answer_question(self, report_content: str, question: str) -> str:
        """Answer a specific question about the report."""
        try:
            qa_prompt = f"""åŸºäºä»¥ä¸‹äº§ä¸šåˆ†ææŠ¥å‘Šï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„ç­”æ¡ˆï¼Œå¹¶åœ¨å¯èƒ½çš„æƒ…å†µä¸‹å¼•ç”¨æŠ¥å‘Šä¸­çš„å…·ä½“å†…å®¹ã€‚

æŠ¥å‘Šå†…å®¹ï¼š
{report_content[:4000]}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸éœ€è¦é¢å¤–çš„æ ¼å¼è¯´æ˜ã€‚"""
            
            if self.llm_service == 'kimi':
                completion = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "user", "content": qa_prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=1000
                )
                return completion.choices[0].message.content
            elif self.llm_service == 'gemini':
                response = self.client.generate_content(qa_prompt)
                return response.text
        
        except Exception as e:
            logger.error(f"Error answering question: {e}")
            return f"æŠ±æ­‰ï¼Œå›ç­”é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
    def _check_key_rotation_and_reinit(self, service_name: str):
        api_keys_cfg = self.config.get('api_keys', {})
        current = None
        if service_name == 'kimi':
            current = (
                api_keys_cfg.get('kimi')
                or api_keys_cfg.get('kimi_api_key')
                or os.environ.get('KIMI_API_KEY')
                or os.environ.get('MOONSHOT_API_KEY')
            )
        elif service_name == 'gemini':
            current = (
                api_keys_cfg.get('google_gemini')
                or api_keys_cfg.get('google_gemini_api_key')
                or os.environ.get('GOOGLE_GEMINI_API_KEY')
            )
        if current and current != getattr(self, 'api_key', None):
            logger.info(f"æ£€æµ‹åˆ° {service_name.upper()} API Key å‘ç”Ÿå˜æ›´ï¼Œæ­£åœ¨é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯...")
            if service_name == 'kimi':
                with self._client_lock:
                    self.client = OpenAI(api_key=current, base_url="https://api.moonshot.cn/v1")
                self.api_key = current
            elif service_name == 'gemini':
                genai.configure(api_key=current)
                with self._client_lock:
                    self.client = genai.GenerativeModel('gemini-1.5-pro-latest')
                self.api_key = current
