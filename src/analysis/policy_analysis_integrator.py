#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Policy Analysis Integration Module
Integrate web scraping with policy analysis and LLM interpretation
"""

import logging
from typing import Dict, List, Optional
from dataclasses import asdict
import json

# Import existing policy analysis functionality
from src.analysis.policy_analyzer import PolicyAnalyzer
from src.data.policy_web_scraper import PolicyWebScraper
from src.ai.llm_generator import LLMReportGenerator
from src.analysis.entity_extractor import EntityExtractor
from src.visualization.knowledge_graph_visualizer import KnowledgeGraphVisualizer


logger = logging.getLogger(__name__)


class PolicyAnalysisIntegrator:
    """
    Integrates web scraping, policy analysis, and LLM interpretation
    """

    def __init__(self, llm_service: str = 'kimi'):
        """Initialize the policy analysis integrator"""
        self.policy_analyzer = PolicyAnalyzer()
        self.scraper = PolicyWebScraper()

        # Initialize LLM generator with error handling
        try:
            self.llm_generator = LLMReportGenerator(llm_service=llm_service, config_path='config.json')
        except ValueError as e:
            if "API key not found" in str(e):
                logger.warning(f"LLM API key not configured: {e}")
                self.llm_generator = None
            else:
                raise e

        self.entity_extractor = EntityExtractor()
        self.graph_visualizer = KnowledgeGraphVisualizer()

    def analyze_policy_from_url(self, url: str, company_profile: Optional[Dict] = None) -> Dict:
        """
        Analyze policy from URL by scraping content and running analysis
        
        Args:
            url: URL to analyze
            company_profile: Optional company profile for applicability assessment
            
        Returns:
            Complete analysis result
        """
        try:
            logger.info(f"ğŸ” Analyzing policy from URL: {url}")
            
            # Step 1: Scrape the policy content
            scraped_result = self.scraper.scrape_policy_content(url)
            if not scraped_result or scraped_result.get('status') != 'success':
                error_msg = scraped_result.get('error', 'Unknown error') if scraped_result else 'Scraping failed'
                logger.error(f"âŒ Failed to scrape content from {url}: {error_msg}")
                return {
                    'success': False,
                    'error': f'Scraping failed: {error_msg}',
                    'url': url
                }
            
            content = scraped_result['content']
            title = scraped_result['title']
            
            if not content.strip():
                logger.error(f"âŒ No content found at {url}")
                return {
                    'success': False,
                    'error': 'No content found on the page',
                    'url': url
                }
            
            # Step 2: Extract entities
            logger.info("ğŸ“Š Extracting entities...")
            entities = self.entity_extractor.extract_entities(content)
            
            # Step 3: Analyze policy content
            logger.info("ğŸ“‹ Analyzing policy content...")
            policy_analysis = self.policy_analyzer.analyze_policy(content, company_profile)
            
            # Step 4: Generate knowledge graph
            logger.info("ğŸŒ Generating knowledge graph...")
            graph_data = self.graph_visualizer.transform_entities_to_graph(entities)
            echarts_config = self.graph_visualizer.generate_echarts_config(
                graph_data, f"{title} - å®ä½“å…³ç³»å›¾"
            )
            
            # Step 5: Generate LLM interpretation
            logger.info("ğŸ¤– Generating LLM interpretation...")
            llm_interpretation = self._generate_llm_interpretation(content, title)

            # Step 5.5: Enhance policy analysis if LLM interpretation is available
            if llm_interpretation and 'error' not in llm_interpretation:
                # Update the policy analysis with LLM insights
                if 'summary' in llm_interpretation and llm_interpretation['summary']:
                    if 'summary' not in policy_analysis or not policy_analysis['summary']:
                        policy_analysis['summary'] = {'highlights': [], 'subsidies_and_taxes': {}, 'timeline': [], 'statistics': {}}
                    # Add LLM summary to policy analysis
                    policy_analysis['summary']['llm_summary'] = llm_interpretation['summary']
            
            # Step 6: Extract classification info
            logger.info("ğŸ·ï¸  Extracting classification info...")
            classification_info = self._extract_classification_info(content, title)
            
            # Step 7: Compile complete result
            result = {
                'success': True,
                'url': url,
                'title': title,
                'content': content,
                'scraped_metadata': scraped_result.get('metadata', {}),
                'policy_analysis': policy_analysis,
                'entities': entities,
                'knowledge_graph': {
                    'data': graph_data,
                    'echarts_config': echarts_config
                },
                'llm_interpretation': llm_interpretation,
                'classification': classification_info,
                'analyzed_at': scraped_result.get('scraped_at')
            }
            
            logger.info(f"âœ… Successfully analyzed policy: {title[:50]}...")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error analyzing policy from {url}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'success': False,
                'error': f'Analysis error: {str(e)}',
                'url': url
            }

    def _generate_llm_interpretation(self, content: str, title: str) -> Dict:
        """Generate LLM-based interpretation of the policy"""
        try:
            from src.ai.llm_generator import LLMReportGenerator

            # Load config directly to check for API keys without initializing the full generator
            import json
            import os
            from pathlib import Path

            # Try to load config directly
            config_path = Path('config.json')
            if not config_path.exists():
                logger.error("Config file not found at config.json")
                return {
                    'summary': 'é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°',
                    'key_points': ['é…ç½®é—®é¢˜ï¼šconfig.jsonæ–‡ä»¶ä¸å­˜åœ¨'],
                    'support_measures': [],
                    'application_conditions': [],
                    'timeline': [],
                    'industry_impact': [],
                    'recommendations': []
                }

            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            api_keys = config.get('api_keys', {})

            # Check if Kimi API key is available
            kimi_key = (
                api_keys.get('kimi')
                or api_keys.get('kimi_api_key')
                or os.environ.get('KIMI_API_KEY')
                or os.environ.get('MOONSHOT_API_KEY')
            )

            if not kimi_key:
                logger.warning("Kimi API key not found in config, skipping LLM analysis")
                return {
                    'summary': 'LLMè§£è¯»æœåŠ¡æœªé…ç½®',
                    'key_points': ['è¦ç‚¹åˆ†æå¾…é…ç½®LLMæœåŠ¡åå¯ç”¨'],
                    'support_measures': ['æ”¯æŒæªæ–½åˆ†æå¾…é…ç½®LLMæœåŠ¡åå¯ç”¨'],
                    'application_conditions': ['ç”³è¯·æ¡ä»¶åˆ†æå¾…é…ç½®LLMæœåŠ¡åå¯ç”¨'],
                    'timeline': ['æ—¶é—´ä¿¡æ¯åˆ†æå¾…é…ç½®LLMæœåŠ¡åå¯ç”¨'],
                    'industry_impact': ['äº§ä¸šå½±å“åˆ†æå¾…é…ç½®LLMæœåŠ¡åå¯ç”¨'],
                    'recommendations': ['ç”³æŠ¥å»ºè®®åˆ†æå¾…é…ç½®LLMæœåŠ¡åå¯ç”¨']
                }

            # Now initialize the LLM generator knowing that the key exists
            llm_gen = LLMReportGenerator(llm_service='kimi', config_path='config.json')

            # Prepare context for LLM
            context = f"""
            è¯·å¯¹ä»¥ä¸‹æ”¿ç­–æ–‡ä»¶è¿›è¡Œæ·±åº¦è§£è¯»å’Œåˆ†æï¼š

            æ”¿ç­–æ ‡é¢˜ï¼š{title}

            æ”¿ç­–å†…å®¹ï¼š
            {content[:4000]}  # Limit content to avoid token issues

            è¯·æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š

            1. æ”¿ç­–è¦ç‚¹æ€»ç»“ (Executive Summary)
            - æ ¸å¿ƒç›®æ ‡
            - ä¸»è¦æ”¯æŒæ–¹å‘
            - é‡è¦æ•°æ®æŒ‡æ ‡

            2. å…³é”®æ”¯æŒæªæ–½ (Support Measures)
            - èµ„é‡‘æ”¯æŒï¼šå…·ä½“é‡‘é¢ã€æ¯”ä¾‹
            - ç¨æ”¶ä¼˜æƒ ï¼šå…·ä½“ç¨ç‡ã€å‡å…å¹…åº¦
            - å…¶ä»–æ”¯æŒï¼šå¦‚åœŸåœ°ã€äººæ‰ã€æŠ€æœ¯ç­‰

            3. é€‚ç”¨æ¡ä»¶ (Application Conditions)
            - ä¼ä¸šèµ„è´¨è¦æ±‚
            - è¡Œä¸šé¢†åŸŸé™åˆ¶
            - åœ°åŸŸè¦æ±‚
            - å…¶ä»–æ¡ä»¶

            4. é‡è¦æ—¶é—´èŠ‚ç‚¹ (Timeline)
            - æ”¿ç­–ç”Ÿæ•ˆæ—¶é—´
            - ç”³æŠ¥æˆªæ­¢æ—¶é—´
            - é¢„æœŸå®Œæˆæ—¶é—´

            5. å½±å“åˆ†æ (Impact Analysis)
            - å¯¹ç›¸å…³äº§ä¸šçš„å½±å“
            - å¯¹ä¼ä¸šçš„å½±å“
            - é¢„æœŸæ•ˆæœ

            6. ç”³æŠ¥å»ºè®® (Application Recommendations)
            - é€‚åˆçš„ä¼ä¸šç±»å‹
            - å…³é”®ç”³æŠ¥è¦ç‚¹
            - æ³¨æ„äº‹é¡¹

            è¯·ä»¥ç»“æ„åŒ–JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
            """

            # Use the LLM to generate interpretation
            result = llm_gen.generate_report(
                city="æ”¿ç­–åˆ†æ",
                industry="æ”¿ç­–è§£è¯»",
                additional_context=context
            )

            if result.get('success'):
                report_content = result.get('full_content', '')
                # Extract the key information from LLM response
                interpretation = self._parse_llm_response(report_content)
                return interpretation
            else:
                logger.error(f"LLM generation failed: {result.get('error')}")
                # Return basic interpretation if LLM fails
                return {
                    'summary': 'LLMè§£è¯»ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ',
                    'key_points': ['è¦ç‚¹1', 'è¦ç‚¹2', 'è¦ç‚¹3'],
                    'support_measures': ['æ”¯æŒæªæ–½1', 'æ”¯æŒæªæ–½2'],
                    'application_conditions': ['ç”³è¯·æ¡ä»¶1', 'ç”³è¯·æ¡ä»¶2'],
                    'timeline': ['æ—¶é—´èŠ‚ç‚¹1', 'æ—¶é—´èŠ‚ç‚¹2'],
                    'industry_impact': ['å½±å“1', 'å½±å“2'],
                    'recommendations': ['å»ºè®®1', 'å»ºè®®2']
                }

        except Exception as e:
            logger.error(f"âŒ Error generating LLM interpretation: {e}")
            # Return a user-friendly response when LLM is not available
            return {
                'summary': 'LLMè§£è¯»æœåŠ¡ä¸å¯ç”¨',
                'key_points': ['æ”¿ç­–å…³é”®è¦ç‚¹éœ€é…ç½®LLMæœåŠ¡åè‡ªåŠ¨ç”Ÿæˆ'],
                'support_measures': ['èµ„é‡‘æ”¯æŒæªæ–½éœ€é…ç½®LLMæœåŠ¡åè‡ªåŠ¨ç”Ÿæˆ'],
                'application_conditions': ['ç”³è¯·æ¡ä»¶éœ€é…ç½®LLMæœåŠ¡åè‡ªåŠ¨ç”Ÿæˆ'],
                'timeline': ['æ—¶é—´è¦æ±‚éœ€é…ç½®LLMæœåŠ¡åè‡ªåŠ¨ç”Ÿæˆ'],
                'industry_impact': ['äº§ä¸šå½±å“åˆ†æéœ€é…ç½®LLMæœåŠ¡åè‡ªåŠ¨ç”Ÿæˆ'],
                'recommendations': ['ç”³æŠ¥å»ºè®®éœ€é…ç½®LLMæœåŠ¡åè‡ªåŠ¨ç”Ÿæˆ']
            }

    def _parse_llm_response(self, response_text: str) -> Dict:
        """Parse LLM response to extract structured information"""
        try:
            # Handle case where response_text is None
            if not response_text:
                logger.warning("LLM response is empty or None")
                return {
                    'summary': 'æ”¿ç­–è§£è¯»',
                    'key_points': ['è¦ç‚¹1', 'è¦ç‚¹2', 'è¦ç‚¹3'],
                    'support_measures': ['æ”¯æŒæªæ–½'],
                    'application_conditions': ['ç”³è¯·æ¡ä»¶'],
                    'timeline': ['æ—¶é—´ä¿¡æ¯'],
                    'industry_impact': ['å½±å“åˆ†æ'],
                    'recommendations': ['ç”³æŠ¥å»ºè®®']
                }

            # Since the LLM might return unstructured text, we'll extract key information using regex
            import re

            # Extract summary
            summary_matches = re.search(r'æ‘˜è¦|æ€»ç»“|æ¦‚è¦[ï¼š:](.*?)(?=å…³é”®æ”¯æŒæªæ–½|é€‚ç”¨æ¡ä»¶|æ—¶é—´èŠ‚ç‚¹|$)', response_text, re.DOTALL | re.IGNORECASE)
            summary = summary_matches.group(1).strip() if summary_matches else "æ”¿ç­–è§£è¯»æ‘˜è¦"

            # Extract key points
            key_points_matches = re.findall(r'[â€¢â—\-](.+?)(?=\n|$)', response_text[:1000])  # First 1000 chars for key points
            key_points = [kp.strip() for kp in key_points_matches[:10] if kp and kp.strip()]  # Top 10 points

            # Extract support measures
            support_pattern = r'æ”¯æŒæªæ–½|èµ„é‡‘æ”¯æŒ|ç¨æ”¶ä¼˜æƒ [ï¼š:](.*?)(?=é€‚ç”¨æ¡ä»¶|æ—¶é—´èŠ‚ç‚¹|å½±å“åˆ†æ|$)'
            support_matches = re.search(support_pattern, response_text, re.DOTALL | re.IGNORECASE)
            support_measures = [support_matches.group(1)[:200].strip()] if support_matches and support_matches.group(1) else ["æ”¯æŒæªæ–½ä¿¡æ¯"]

            # Extract application conditions
            condition_pattern = r'é€‚ç”¨æ¡ä»¶|ç”³æŠ¥æ¡ä»¶|èµ„æ ¼è¦æ±‚[ï¼š:](.*?)(?=æ—¶é—´èŠ‚ç‚¹|å½±å“åˆ†æ|ç”³æŠ¥å»ºè®®|$)'
            condition_matches = re.search(condition_pattern, response_text, re.DOTALL | re.IGNORECASE)
            application_conditions = [condition_matches.group(1)[:200].strip()] if condition_matches and condition_matches.group(1) else ["ç”³è¯·æ¡ä»¶ä¿¡æ¯"]

            # Extract timeline
            timeline_pattern = r'æ—¶é—´èŠ‚ç‚¹|æ—¶é—´è¦æ±‚|æˆªæ­¢æ—¶é—´[ï¼š:](.*?)(?=å½±å“åˆ†æ|ç”³æŠ¥å»ºè®®|$)'
            timeline_matches = re.search(timeline_pattern, response_text, re.DOTALL | re.IGNORECASE)
            timeline = [timeline_matches.group(1)[:200].strip()] if timeline_matches and timeline_matches.group(1) else ["æ—¶é—´ä¿¡æ¯"]

            # Extract industry impact
            impact_pattern = r'å½±å“åˆ†æ|äº§ä¸šå½±å“|é¢„æœŸæ•ˆæœ[ï¼š:](.*?)(?=ç”³æŠ¥å»ºè®®|æ€»ç»“|$)'
            impact_matches = re.search(impact_pattern, response_text, re.DOTALL | re.IGNORECASE)
            industry_impact = [impact_matches.group(1)[:200].strip()] if impact_matches and impact_matches.group(1) else ["å½±å“åˆ†æ"]

            # Extract recommendations
            rec_pattern = r'ç”³æŠ¥å»ºè®®|æ³¨æ„äº‹é¡¹|å»ºè®®[ï¼š:](.*?)(?=$|\n\n)'
            rec_matches = re.search(rec_pattern, response_text, re.DOTALL | re.IGNORECASE)
            recommendations = [rec_matches.group(1)[:200].strip()] if rec_matches and rec_matches.group(1) else ["ç”³æŠ¥å»ºè®®"]

            return {
                'summary': summary[:500],  # Limit length
                'key_points': key_points or ['æ”¿ç­–å…³é”®è¦ç‚¹'],
                'support_measures': support_measures,
                'application_conditions': application_conditions,
                'timeline': timeline,
                'industry_impact': industry_impact,
                'recommendations': recommendations
            }
        except Exception as e:
            logger.error(f"Error parsing LLM response: {e}")
            # Return basic structure if parsing fails
            return {
                'summary': 'æ”¿ç­–è§£è¯»',
                'key_points': ['è¦ç‚¹1', 'è¦ç‚¹2', 'è¦ç‚¹3'],
                'support_measures': ['æ”¯æŒæªæ–½'],
                'application_conditions': ['ç”³è¯·æ¡ä»¶'],
                'timeline': ['æ—¶é—´ä¿¡æ¯'],
                'industry_impact': ['å½±å“åˆ†æ'],
                'recommendations': ['ç”³æŠ¥å»ºè®®']
            }

    def _extract_classification_info(self, content: str, title: str) -> Dict:
        """Extract classification info (region, industry, year)"""
        classification = {
            'region': self._extract_regions(content, title),
            'industry': self._extract_industries(content),
            'year': self._extract_year(content, title),
            'policy_type': self._determine_policy_type(content, title)
        }
        
        return classification

    def _extract_regions(self, content: str, title: str) -> List[str]:
        """Extract regions mentioned in the policy"""
        # Common region patterns
        region_patterns = [
            r'(?:åœ¨|å¯¹|æ”¯æŒ|é’ˆå¯¹)(.*?)(?:å¸‚|çœ|åŒº|å¿|å·|åœ°åŒº|è‡ªæ²»åŒº|ç›´è¾–å¸‚)',
            r'(.*?)(?:å¸‚|çœ|åŒº|å¿|å·|åœ°åŒº|è‡ªæ²»åŒº|ç›´è¾–å¸‚)(?:å‘å¸ƒ|å®æ–½|å‡ºå°)',
            r'(.*?)(?:å¸‚|çœ|åŒº|å¿|å·|åœ°åŒº|è‡ªæ²»åŒº|ç›´è¾–å¸‚)[\u4e00-\u9fa5]*æ”¿ç­–',
        ]
        
        import re
        regions = set()
        
        # Add title and content together for better extraction
        full_text = f"{title} {content}"
        
        for pattern in region_patterns:
            matches = re.findall(pattern, full_text)
            for match in matches:
                if len(match) <= 10:  # Reasonable length for region names
                    regions.add(match.strip())
        
        # Add common regions that might be mentioned differently
        common_regions = [
            "åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æˆéƒ½", "é‡åº†", "æ­å·", "å—äº¬", "æ­¦æ±‰", "è¥¿å®‰",
            "è‹å·", "å¤©æ´¥", "é’å²›", "å¤§è¿", "å®æ³¢", "å¦é—¨", "å¹¿å·", "æ·±åœ³", "æˆéƒ½", "è¥¿å®‰"
        ]
        
        for region in common_regions:
            if region in full_text:
                regions.add(region)
        
        return list(regions)[:5]  # Return top 5 regions

    def _extract_industries(self, content: str) -> List[str]:
        """Extract industries mentioned in the policy"""
        # Common industry keywords
        industry_keywords = [
            "äººå·¥æ™ºèƒ½", "å¤§æ•°æ®", "äº‘è®¡ç®—", "ç‰©è”ç½‘", "5G", "åŒºå—é“¾", "æ–°èƒ½æº", 
            "ç”Ÿç‰©åŒ»è¯", "æ–°ææ–™", "é«˜ç«¯åˆ¶é€ ", "æ•°å­—ç»æµ", "æ™ºèƒ½åˆ¶é€ ", "é›†æˆç”µè·¯",
            "æ–°èƒ½æºæ±½è½¦", "ç”Ÿç‰©åŒ»è¯", "æ–°ææ–™", "èˆªç©ºèˆªå¤©", "ç°ä»£æœåŠ¡ä¸š", "ç°ä»£å†œä¸š",
            "ä¿¡æ¯æŠ€æœ¯", "ç”Ÿç‰©æŠ€æœ¯", "æ–°ææ–™", "æ–°èƒ½æº", "é«˜ç«¯è£…å¤‡åˆ¶é€ ", "èŠ‚èƒ½ç¯ä¿"
        ]
        
        industries = set()
        
        for keyword in industry_keywords:
            if keyword in content:
                industries.add(keyword)
        
        return list(industries)

    def _extract_year(self, content: str, title: str) -> Optional[int]:
        """Extract the policy year"""
        import re
        
        # Look for 4-digit year patterns
        year_patterns = [
            r'(?:å‘å¸ƒ|å®æ–½|å‡ºå°|æ‰§è¡Œ)äº?(\d{4})å¹´',
            r'(\d{4})å¹´(\d{1,2})æœˆ',
            r'(\d{4})å¹´',
            r'(\d{4})-(?:\d{1,2})-(?:\d{1,2})'
        ]
        
        full_text = f"{title} {content}"
        
        for pattern in year_patterns:
            match = re.search(pattern, full_text)
            if match:
                year = int(match.group(1))
                if 1900 <= year <= 2030:  # Reasonable year range
                    return year
        
        return None

    def _determine_policy_type(self, content: str, title: str) -> str:
        """Determine the policy type"""
        type_indicators = {
            "æ‰¶æŒæ”¿ç­–": ["æ‰¶æŒ", "èµ„åŠ©", "è¡¥è´´", "å¥–åŠ±", "æ”¯æŒ"],
            "ç¨æ”¶ä¼˜æƒ ": ["ç¨æ”¶", "å‡å…", "ä¼˜æƒ ", "å‡ç¨", "å…ç¨"],
            "å‡†å…¥æ”¿ç­–": ["å‡†å…¥", "è®¸å¯", "å®¡æ‰¹", "èµ„è´¨", "é—¨æ§›"],
            "ç›‘ç®¡æ”¿ç­–": ["ç›‘ç®¡", "è§„èŒƒ", "æ•´é¡¿", "æ²»ç†", "ç®¡ç†"],
            "å‘å±•è§„åˆ’": ["è§„åˆ’", "è®¡åˆ’", "çº²è¦", "æ–¹æ¡ˆ", "å¸ƒå±€"]
        }
        
        title_content = f"{title} {content}".lower()
        
        for policy_type, keywords in type_indicators.items():
            for keyword in keywords:
                if keyword in title_content:
                    return policy_type
        
        return "å…¶ä»–æ”¿ç­–"

    def batch_analyze_policies(self, urls: List[str], company_profile: Optional[Dict] = None) -> List[Dict]:
        """Analyze multiple policy URLs"""
        results = []
        
        for i, url in enumerate(urls):
            logger.info(f"Progress: {i+1}/{len(urls)} - Analyzing: {url}")
            
            result = self.analyze_policy_from_url(url, company_profile)
            results.append(result)
        
        return results

    def generate_policy_summary(self, analysis_result: Dict) -> Dict:
        """Generate a comprehensive summary of policy analysis"""
        try:
            policy_analysis = analysis_result.get('policy_analysis', {})
            summary_data = policy_analysis.get('summary', {})
            
            summary = {
                "title": analysis_result.get('title', 'Unknown'),
                "highlights_count": summary_data.get('statistics', {}).get('total_highlights', 0),
                "subsidies_count": summary_data.get('statistics', {}).get('total_subsidies', 0),
                "tax_benefits_count": summary_data.get('statistics', {}).get('total_tax_benefits', 0),
                "upcoming_deadlines": summary_data.get('statistics', {}).get('upcoming_deadlines', 0),
                "regions": analysis_result.get('classification', {}).get('region', []),
                "industries": analysis_result.get('classification', {}).get('industry', []),
                "year": analysis_result.get('classification', {}).get('year'),
                "policy_type": analysis_result.get('classification', {}).get('policy_type'),
                "applicability_score": policy_analysis.get('applicability', {}).get('score', 0) if policy_analysis.get('applicability') else 0,
                "key_subsidies": [s['description'][:50] for s in summary_data.get('subsidies_and_taxes', {}).get('subsidies', [])[:3]],
                "key_deadlines": [t['date'] for t in summary_data.get('timeline', [])[:3] if t.get('is_future')],
            }
            
            return summary
        except Exception as e:
            logger.error(f"Error generating policy summary: {e}")
            return {"error": str(e)}


if __name__ == "__main__":
    # Test the integrator
    logging.basicConfig(level=logging.INFO)
    
    integrator = PolicyAnalysisIntegrator()
    
    # Test with a sample URL (this would be from email)
    test_url = "https://example.com/sample-policy"
    
    print(f"Testing policy analysis for: {test_url}")
    result = integrator.analyze_policy_from_url(test_url)
    
    if result.get('success'):
        print(f"âœ… Analysis successful!")
        print(f"Title: {result.get('title', 'N/A')}")
        print(f"Regions: {result.get('classification', {}).get('region', [])}")
        print(f"Industries: {result.get('classification', {}).get('industry', [])}")
        print(f"Year: {result.get('classification', {}).get('year')}")
        print(f"Policy Type: {result.get('classification', {}).get('policy_type')}")
    else:
        print(f"âŒ Analysis failed: {result.get('error', 'Unknown error')}")
    
    print("\nâœ… Policy analysis integrator module ready!")